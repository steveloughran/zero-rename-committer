%report.tex
% the glue for everything else
\documentclass[9pt,technote]{IEEEtran}
%\documentstyle{report}
\renewcommand{\baselinestretch}{1.5}
\begin{document}
\parindent 0pt
\setlength{\parskip}{3ex}



\title{Zero-rename output committers for working with object storesApache Hadoop and Spark for object storage}
\author{Steve Loughran}
\maketitle


\section{Abstract}

We introduce new \emph{committers} for Apache Hadoop, so permitting
the Amazon S3 Object Store to be used as a direct destination of output generated
by Hadoop MapReduce and Apache Spark.

By using the operations directly exported by
the store, most critically the multipart upload mechanism, these committers upload
their output to the final destination, yet do not materialize this data until the
overall job is committed.
As a result, the committers meet the core requirement of the Hadoop and Spark commit
protocols: output is not visible until committed, while being highly-performant.

We also define the commit protocols of Hadoop and Spark, and show how the classic committer implementation's requirements of atomic file creation and rename operations mean that they cannot be safely used with Amazon S3.
Thus the new committers are less ``better'' committers for object stores, but rather ``safe for use''.

We discuss the two committers, ``Staging'' and ``Magic'', exploring their differences.
The Staging committer stages all generated output to the local filesystem of  worker nodes, uploading this data when a task is committed.
The Magic committer streams data directly to the object store, relying on the object store client to recognise some output paths as special (``magic''), and so translating writes to these paths as initiating a delayed-completion write to a calculated final destination.



\section{Introduction}
It has long been a core requirement of ``Big Data'' computation platforms that the source and destination of data was a fully consistent distributed filesystem, albeit often ``sub-Posix''.
A distributed filesystem, because data needs to be visible to all processes executing a query spanning multiple machines in a cluster of computers.
``Sub-Posix'', in that the full semantics of the POSIX filesystem APIs were not always necessary.
A commonly dropped requirement is the ability to seek past the end of the file and write data, with a secondary dropped behavior one of being able to overwrite existing data within a file.
That is: new data could only be written at the current end of the file.
What has been a consistent part of the required semantics has been that the filesystem presents a model of directories and files with consistent operations to list and read those directories, and at least four atomic operations:
\begin{itemize}
  \item{rename of a single file to another location within the same volume}
  \item{rename of a directory and its contents to another location within the same volume}
  \item{create file with no overwrite}
  \item{recursive delete of a directory}
\end{itemize}
These operations are often used as the base operations of higher-level co-ordination and commit protocols.
For example, the create operation can be used to obtain a lock on a resource: the first process to create a file can consider itself having exclusive access to it, and so implicitly consider itself to have acquire a resource.
The \texttt{rename()} operation is generally critical to providing atomic promotion of output: a single \texttt{rename()} call can promote all in-progress output of a worker to become completed work, simply by moving all the output to a well known path.
And, when the job completes, its final output may be renamed to a new location to become publicly visible.
\begin{quote}
  We rely on the atomic rename operation provided by the underlying file system to guarantee that the final file system state contains just the data produced by one execution of the reduce task.
\end{quote}
\cite{Dean:2004:MSD:1251254.1251264}

Object stores do not generally offer the full semantics of a filesystem. Generally the set of operations available are an extended set of HTTP verbs
\begin{itemize}
  \item{\texttt{PUT path, data}: write an object, with the output only becoming visible when the call successfully completes.}
  \item{\texttt{GET path}}
  \item{\texttt{HEAD path}}
  \item{\texttt{LIST path}}
  \item{\texttt{COPY path, newpath} copy a single object within the store, completion time a function size/bandwidth}
  \item{\texttt{DELETE path}  + a bulk delete call which may have partial failures}
  \item{optionally: a way to PUT an object larger than 5GB through a series of PUT/POST operations and a final PUT/POST to complete the sequence.}
\end{itemize}

Object stores which lack an atomic O(1) \texttt{rename()} operation cannot be safely used as the direct destination of work from Apache Hadoop MapReduce or Apache Spark, due to the existing ``Committer''s dependency upon rename for atomic commit operations.
If the object store is eventually consistent, then even mimicing the rename() operation is at risk of losing data.
Thus it is neither fast nor reliable.


\section{Hadoop and object stores}

Apache Hadoop was written with its own filesystem, Hadoop Distributed File System (HDFS).

It is self-admittedly "sub-POSIX", specifically, while presenting the POSIX
model of a filesystem tree containing directories and files, data can only be
appended directly to the end of the current file.
That is: one my not \texttt{seek()} to before or after the end of the file and
then write data.

It does offer atomic operations which are used in commit protocols and other
applications as a means of enforcing exclusivity; so permitting the filesystem
to act as a mechanism of co-ordinating access across machines.

% As all filesystem
%operations are via the NameNode, all clients get a consistent view of the filesystem.
%And, as the




\section{Limitations}

A key criticism of the new algorithm is that the job commit operation is not atomic;
it is \emph{O(files)} operation which may fail partway through. We respond that as Hadoop's MapReduce V1 commit algorithm it itself non-atomic in job commit,

An unrelated problem, but one which may manifest itself in cloud-based deployments, is that the higher-level commit protocol assumes that time increases monotonically on individual machines in the cluster.
The job manager and workers can use the interval between the last successful hearbeat and the current time as the means by which they can consider themselves to have lost contact with each other and system services.
In cloud environments clocks may stutter, proceed at significatly different rates, and indeed, may even proceed backwards \cite{anything?}.

We offer no solution to this, except to question whether virtualized clocks can be considered a reliable source of \emph{time}, and ask: what can be used instead?
Given that all processes are working with a shared object store, it may be simplest to use this as an approximate source of time, at least to the extent of using its timestamps as a normative source of increasing time values which can be consistently viewed across all workers.
Yet that itself is a dangerous assumption to make.


\section{Related Work}

Apache spark (briefly) offered a zero rename committer, the \emph{Direct Output Committer}\cite{JIRA}.
With this committer, output was written directly to the destination directory;
both task and job commit operations were reduced to no-ops.
To avoid concurrency issues, speculative execution of tasks was automatically disabled when this committer was used.
Unfortunately, the committer was still not resilient to failure: a failed task could not be repeated, as its output was unknown.
For this reason it was discontinued \cite{DOC-JIRA}.
It's absence is now noted by users, showing how a much 0-rename committer was valued by users, even if it failed to offer any of the actual semantics of a commit protocol.
Alternatively: performance is observable, whereas consistency and failures are not considered important until they surface in production systems.

IBM stocator eliminates renames by also having a direct write to the destination  \cite{DBLP:journals/corr/abs-1709-01812}.
As with the \emph{the Magic Committer}, it modifies the semantics of write operations into the temporary directories of work, here the \texttt{\_temporary} path used by the classic \texttt{FileOutputCommitter}.
To avoid the failure semantics of Spark's \emph{Direct Output Committer}, every remapped file was given a name which added the job and task attempt IDs, while still preserving the sort order.
As a result, failed and aborted tasks and jobs could be cleaned up by their successors.
Stocator also generated a JSON-formatted \texttt{\_SUCCESS} file, which offered the ability to obtain a consistent list of the final files committed by a job, even in the presence of listing inconsistency.
With this design, Stocator does make the output of work immediately visible;
again there is no task commit, and the job commit is a matter of writing the \texttt{\_SUCCESS} file.
This is achieved by misleading the classic committer, which believes that it is writing files to a temporary destination and renamining them.
The closest of the two S3A committers is the magic committer.
It too writes the output to a different destination than the location passed to the \texttt{FileOutputFormat} instance writing data.
However, the data is not actually manifest until the final job is committed: there is no observable change to the destination directory until the job commit.
Thus it provides the standard semantics of task and job commmit: no data is visible until the job is committed.



\section{Acknowledgements}

\section{References}

% Bibliography. Include
% Expects the formality project to be alongside this one.

\bibliography{./formality/papers/bibliography/bibliography}

S3Guard
S3mper
Spark direct committer JIRAs
A history of Hadoop's object store => research gate?

\end{document}
