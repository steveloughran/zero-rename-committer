\documentclass{article}
% ========================================================================
\begin{document}

What can be done for improving commit operations

\section{Hadoop}\label{sec:hadoop}

Add check for dest dir existing before job setup?
Currently there's a window between job submit and OutputCommitter.setupJob()
where no attempt to create the dest dir exists.

The *first* attempt could reject if the dest dir is there;
later ones should expect it.

\section{Spark}\label{sec:spark}

Handling on task attempt commit timeout

\begin{enumerate}
  \item choose whether to commit another attempt on task commit timeout based on committer attributes.
  \item Short term: if FileOutputCommitter, check algorithm and reject restart if v2
  \item Long term: add predicate to PathOutputCommitter.
\end{enumerate}

Avoid duplicate job attempt commmit after a partitioned spark driver

Q1: is this actually a problem on YARN?

How would a test simulate this?

Q2: what about on Mesos or YARN with driver running off-cluster?

Q3: How to address in a infra-neutral way?
Need to query infra for last time a heartbeat took place and wait until
its within a time range before committing.


\texttt{newTaskTempFileAbsPath()} is, well, trouble for the new committers.
It could be supported in the staging committers again throue upload in task commit.

For the magic committer, we could add a new prefix under \texttt{__magic},
\texttt{__root}, alongside \texttt{__base}.
The magic path mapping logic would then be "when writing to a magic path
\texttt{dest/__magic/jobAttemptId/taskAttemptID/__root/abspath}, then the destination of the write
would actually become "abspath".
The existing commit algorithm would automatically incorporate the .pending files
created in the write, and so commit them along with the relative paths.
There is one sole complication: with the writes scattered through the S3 bucket,
a full cleanup would require listing and aborting all outstanding MPUs
writes throughout the bucket.
This could not be done without breaking all other active jobs writing to
the same bucket.


Committer resilience/reliability could be enhanced by tracking that list of
files to commit: tasks could record the files they'd provisionally committed and return
that to the job committer. (more specifically, final dest files)
That could then verify that after job commit, they were there.
This is a safety check which could be done at the commit layer; the spark protocols
have room for this


\section{Statistic collection}

A long term fix woud be to radically improve the means by which statistics are
collected by tasks and then aggregated in the job committer.
The latest versions of the HDFS and object store connectors are heavily instrumented,
collecting lots of information on filesystem client use.
for S3A this includes: times an HTTP1.1 connection was aborted on a file read,
the number of times a S3 request was throttled and retried,
even some latency statistics.
All of these would be useful if they could be collected by the job, aggregated
appropriately, and included in the job execution history.
It is possible to collect this information, and then pass this back.
Indeed, our committers do collect this data themselves, saving it in the
manifest files listing pending uploads, combinining this into the \SUCCESS file
in job commit.
This data is not yet extracted by the job committer and returned to the execution
environment.
This can be fixed.

\section{Committer Capabilities}

Committers should be able to declare that they have certain attributes, such
as repeatable task commit operations (currently only job commit is declared).

Rather than propose a continually growing list of predicates, we'd argue for
implementing the \texttt{org.apache.hadoop.fs.StreamCapabilities} interface
and its \texttt{boolean hasCapability(String capability)} predicate, with capabilities
chosen so that the default return value, false, is the pessimistic outcome.
By reusing the capabilities probe already built into Hadoop 2.9+, it's  easier
to adopt the checks.



\section{S3a Committers}


\section{Validation}

What about an option of a post Job commit verification that all committed files
are really there?
That's done in the integration tests by reading the data in _SUCCESS;
it could be added into the Job Committer itself.
It's potentially expensive in cost and time at a HEAD/file;
a listFiles(dest, recursive) could be used instead, with, on the partitioned
committer, only generated partitions checked.

This is really just a safety check, but it could could be useful in testing
and diagnostics, because you wouldn't need to add specific logic to read in
the (unstable) _SUCCESS JSON data & check there.

\subsubsection{Magic committer}

Make sure that an empty directory can still be committed & generates
an empty pendingset  file, and that when loaded, its harmless.



\section{Aborts and cleanup}

Everythuing needs to assume that the committers abort and cleanup operations may raise exceptions.

When invoked, they should be should be surrounded by try/catch blocks When subclassed, the subclasses
should do their own cleanup operations in a try/finally clause.

Executors/drivers should call these operations on all failure codepaths;
Hadoop MR doesn't appear to do this.

Current committers should do that try/catch themselves, even though the method
signature allows them to raise IOEs.
This avoids them having to wait for new releases of the execution engines
for the add resilience.


\end{document}
